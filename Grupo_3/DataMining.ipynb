{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, parse_qs\n",
    "import zipfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the page containing the links to the ZIP files\n",
    "base_url = \"https://www.omie.es\"\n",
    "page_url = \"https://www.omie.es/en/file-access-list?parents%5B0%5D=/&parents%5B1%5D=Day-ahead%20Market&parents%5B2%5D=3.%20Curves&dir=Monthly%20files%20with%20aggregate%20supply%20and%20demand%20curves%20of%20Day-ahead%20market%20including%20bid%20units&realdir=curva_pbc_uof\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where the ZIP files will be saved\n",
    "download_folder = \"data\"\n",
    "extracted_folder = os.path.join(download_folder, \"extracted\")\n",
    "\n",
    "if not os.path.exists(download_folder):\n",
    "    os.makedirs(download_folder)\n",
    "if not os.path.exists(extracted_folder):\n",
    "    os.makedirs(extracted_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get page content\n",
    "response = requests.get(page_url)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error accessing the page: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all links to ZIP files\n",
    "zip_links = []\n",
    "for link in soup.find_all('a'):\n",
    "    href = link.get('href')\n",
    "    if href and \"curva_pbc_uof_\" in href and href.endswith(\".zip\"):\n",
    "        full_url = urljoin(base_url, href)\n",
    "        zip_links.append(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando data\\curva_pbc_uof_202402.zip desde https://www.omie.es/es/file-download?parents%5B0%5D=curva_pbc_uof&filename=curva_pbc_uof_202402.zip\n",
      "Descarga completa: data\\curva_pbc_uof_202402.zip\n",
      "Descargando data\\curva_pbc_uof_202401.zip desde https://www.omie.es/es/file-download?parents%5B0%5D=curva_pbc_uof&filename=curva_pbc_uof_202401.zip\n",
      "Descarga completa: data\\curva_pbc_uof_202401.zip\n",
      "Descargando data\\curva_pbc_uof_202312.zip desde https://www.omie.es/es/file-download?parents%5B0%5D=curva_pbc_uof&filename=curva_pbc_uof_202312.zip\n",
      "Descarga completa: data\\curva_pbc_uof_202312.zip\n",
      "Descargando data\\curva_pbc_uof_202311.zip desde https://www.omie.es/es/file-download?parents%5B0%5D=curva_pbc_uof&filename=curva_pbc_uof_202311.zip\n"
     ]
    }
   ],
   "source": [
    "# Download each ZIP file found\n",
    "for zip_url in zip_links:\n",
    "    # Get file name from URL\n",
    "    parsed_url = urlparse(zip_url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "    zip_name = query_params.get('filename', [None])[0]\n",
    "    if zip_name:\n",
    "        zip_path = os.path.join(download_folder, zip_name)\n",
    "        print(f\"Downloading {zip_path} from {zip_url}\")\n",
    "\n",
    "        # Download ZIP File\n",
    "        response = requests.get(zip_url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(zip_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"Download completed: {zip_path}\")\n",
    "        else:\n",
    "            print(f\"Error downloading {zip_url}: {response.status_code}\")\n",
    "    else:\n",
    "        print(f\"Could not define file name for URL: {zip_url}\")\n",
    "\n",
    "print(\"Download all files completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract and process files\n",
    "def extract_and_process_zip(zip_path):\n",
    "    dataframes = []\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # List ZIP files\n",
    "        for file_name in zip_ref.namelist():\n",
    "            if file_name.endswith(\".1\"):\n",
    "                # Extract file\n",
    "                zip_ref.extract(file_name, extracted_folder)\n",
    "                extracted_file_path = os.path.join(extracted_folder, file_name)\n",
    "                # Read CSV file into a DataFrame\n",
    "                df = pd.read_csv(extracted_file_path, sep=';')\n",
    "                dataframes.append(df)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all ZIP files in the downloads folder\n",
    "zip_files = [f for f in os.listdir(download_folder) if f.endswith('.zip')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajortiz\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3357: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "# Process each ZIP file\n",
    "all_dataframes = []\n",
    "for zip_file in zip_files:\n",
    "    zip_path = os.path.join(download_folder, zip_file)\n",
    "    dataframes = extract_and_process_zip(zip_path)\n",
    "    all_dataframes.extend(dataframes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de archivos extraídos y cargados en DataFrames: 85\n",
      "  OMIE - Mercado de electricidad Fecha Emisi�n :30/11/2023 - 12:59 Unnamed: 2  \\\n",
      "0                           Hora                             Fecha       Pais   \n",
      "1                              1                        01/12/2023         MI   \n",
      "2                              1                        01/12/2023         MI   \n",
      "3                              1                        01/12/2023         MI   \n",
      "4                              1                        01/12/2023         MI   \n",
      "\n",
      "  01/12/2023 Mercado diario            Unnamed: 5           Unnamed: 6  \\\n",
      "0     Unidad    Tipo Oferta  Energ�a Compra/Venta  Precio Compra/Venta   \n",
      "1    TOTRM02              C                 137,6             1.500,00   \n",
      "2    TOTRM01              C                 549,7             1.500,00   \n",
      "3      BPC01              C                  16,9             1.002,00   \n",
      "4     BPRM01              C                  25,3             1.002,00   \n",
      "\n",
      "                Unnamed: 7  Unnamed: 8  \n",
      "0  Ofertada (O)/Casada (C)         NaN  \n",
      "1                        O         NaN  \n",
      "2                        O         NaN  \n",
      "3                        O         NaN  \n",
      "4                        O         NaN  \n"
     ]
    }
   ],
   "source": [
    "# Print the number of extracted DataFrames\n",
    "print(f\"Total de archivos extraídos y cargados en DataFrames: {len(all_dataframes)}\")\n",
    "\n",
    "#You can access each DataFrame in the all_dataframes list\n",
    "# For example, to display the first DataFrame:\n",
    "if all_dataframes:\n",
    "    print(all_dataframes[0].head())\n",
    "\n",
    "# Guardar todos los DataFrames en un solo archivo CSV\n",
    "#combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "#combined_df.to_csv(os.path.join(download_folder, \"combined_data.csv\"), index=False)\n",
    "#print(\"Todos los DataFrames se han combinado y guardado en combined_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
